{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gCtfgH2xywyE",
    "outputId": "8a4210d3-18d8-4a7e-acc2-e6c608df4128",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from pickle import dump, load\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "import keras.utils as ku \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TPlyEduy8pv"
   },
   "outputs": [],
   "source": [
    "def data_preprocess(filename):\n",
    "    \"\"\" preprocess raw data\n",
    "    \"\"\"\n",
    "    poetry = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "        for i, line in enumerate(text):\n",
    "            poetry.append(''.join(line.strip().split(' ')))\n",
    "            if i>10000: break\n",
    "    dump(poetry, open('poetry.pkl','wb')) \n",
    "    return poetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KlaMTwEK0Nb6",
    "outputId": "3bb31838-0802-40fb-f790-322fa493b02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of poems:  10002\n",
      "Example:  秦川雄帝宅函谷壮皇居绮殿千寻起离宫百雉馀连薨遥接汉飞观迥凌虚云日隐层阙风烟出绮疏岩廊罢机务崇文聊驻辇玉匣启龙图金绳披凤篆韦编断仍续缥帙舒还卷对此乃淹留欹案观坟典移步出词林停舆欣武宴雕弓写明月骏马疑流电惊雁落虚弦啼猿悲急箭阅赏诚多美于兹乃忘倦鸣笳临乐馆眺听欢芳节急管韵朱弦清歌凝白雪彩凤肃来仪玄鹤纷成列去兹郑卫声雅音方可悦芳辰追逸趣禁苑信多奇桥形通汉上峰势接云危烟霞交隐映花鸟自参差何如肆辙迹万里赏瑶池飞盖去芳园兰桡游翠渚萍间日彩乱荷处香风举桂楫满中川弦歌振长屿岂必汾河曲方为欢宴所落日双阙昏回舆九重暮长烟散初碧皎月澄轻素搴幌玩琴书开轩引云雾斜汉耿层阁清风摇玉树欢乐难再逢芳辰良可惜玉酒泛云罍兰殽陈绮席千钟合尧禹百兽谐金石得志重寸阴忘怀轻尺璧建章欢赏夕二八尽妖妍罗绮昭阳殿芬芳玳瑁筵佩移星正动扇掩月初圆无劳上悬圃即此对神仙以兹游观极悠然独长想披卷览前踪抚躬寻既往望古茅茨约瞻今兰殿广人道恶高危虚心戒盈荡奉天竭诚敬临民思惠养纳善察忠谏明科慎刑赏六五诚难继四三非易仰广待淳化敷方嗣云亭响\n"
     ]
    }
   ],
   "source": [
    "poetry = data_preprocess('chinese_poetry.txt')\n",
    "print (\"Total # of poems: \", len(poetry))\n",
    "print (\"Example: \", poetry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iw_JF40_0RpJ",
    "outputId": "e46cd5d4-c215-454f-8e72-b79023cbb6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequent:  ('不', 6223) least frequent:  ('稂', 1)\n",
      "total_words: 5664\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for poem in poetry:\n",
    "    all_words += [word for word in poem]\n",
    "counter = Counter(all_words)\n",
    "\n",
    "counter_pairs = sorted(counter.items(), key=lambda s : -s[1])\n",
    "print (\"most frequent: \", counter_pairs[0], \"least frequent: \", counter_pairs[-1])\n",
    "\n",
    "words = [w[0] for w in counter_pairs] + [' ']\n",
    "total_words = len(words)\n",
    "print ('total_words:', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ejgunswl1LWJ",
    "outputId": "9103b419-143c-472f-db46-403182123d0c"
   },
   "outputs": [],
   "source": [
    "# word to number, and poem to vector\n",
    "word_dict = dict(zip(words, range(len(words))))\n",
    "to_num = lambda w: word_dict.get(w, total_words)\n",
    "Lpoetry = [list(map(to_num, poem)) for poem in poetry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "tTjr-MCa24EC",
    "outputId": "1a68e648-d2f0-4e6c-93f0-794c8b933d5c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(sequences):\n",
    "    max_sequence_len = 5\n",
    "    predictors, labels = [], []\n",
    "    for line in sequences:\n",
    "        for i in range(0, len(line)-max_sequence_len):\n",
    "            seq_in = line[i:i+max_sequence_len]\n",
    "            seq_ou = line[i+max_sequence_len]\n",
    "            predictors.append(seq_in)\n",
    "            labels.append(seq_ou)\n",
    "    labels = np_utils.to_categorical(labels, num_classes=total_words)\n",
    "    return np.array(predictors), labels, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(Lpoetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "QnPO7YWM25Xa",
    "outputId": "eca5ee95-ed65-4d98-cbef-3cfec26686ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 10)             56640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5664)              288864    \n",
      "=================================================================\n",
      "Total params: 357,704\n",
      "Trainable params: 357,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "4teu6xrw_fuG",
    "outputId": "8e920a21-8689-446e-8e3d-02eeca1757bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "637049/637049 [==============================] - 241s 378us/step - loss: 7.1541\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.15405, saving model to model.hdf5\n",
      "Epoch 2/3\n",
      "637049/637049 [==============================] - 249s 392us/step - loss: 6.8554\n",
      "\n",
      "Epoch 00002: loss improved from 7.15405 to 6.85541, saving model to model.hdf5\n",
      "Epoch 3/3\n",
      "637049/637049 [==============================] - 250s 392us/step - loss: 6.6686\n",
      "\n",
      "Epoch 00003: loss improved from 6.85541 to 6.66857, saving model to model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a31e5e6d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(predictors, label, epochs=3, callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zu3DTdirCXcM"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, model, max_sequence_len):\n",
    "    text = seed_text\n",
    "    n = len(seed_text)\n",
    "    for _ in range(20-n):\n",
    "        token_list = pad_sequences([seed_text], maxlen=max_sequence_len, padding='post')\n",
    "        predict= model.predict(token_list, verbose=0)\n",
    "        # greedy search without character duplicate\n",
    "        while np.argmax(predict) in text:\n",
    "            predict = np.delete(predict, np.argmax(predict))\n",
    "        text += [np.argmax(predict)]\n",
    "        seed_text = seed_text[1:] + [text[-1]]\n",
    "\n",
    "\n",
    "    output_word = \"\"\n",
    "    for s in text:\n",
    "        for w in word_dict:\n",
    "            if word_dict[w]== s:\n",
    "                output_word += w\n",
    "    return output_word, text\n",
    "\n",
    "def output_poem(num, mod):\n",
    "    ret, score = [], 0\n",
    "    for _ in range(num):\n",
    "        seed = random.randint(0,len(Lpoetry))\n",
    "        try:\n",
    "            seed_text = Lpoetry[seed][:3]\n",
    "        except:\n",
    "            pass\n",
    "        p, t= generate_text(seed_text, mod, max_sequence_len)\n",
    "        s = sentence_bleu(Lpoetry, t)\n",
    "        ret.append(p)\n",
    "        score += s\n",
    "    return ret, score/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output generated poems.\n",
    "p, s = output_poem(4, model)\n",
    "for i in range(len(p)):\n",
    "    print ('\\n', p[i][:5],', ',p[i][5:10],'.\\n', p[i][10:15],', ', p[i][15:],'.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., \\\n",
    "                                 2 * K.arange(self.size / 2, dtype='float32' \\\n",
    "                               ) / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, mask_right=False, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.mask_right = mask_right\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1)) \n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            mask = (ones - K.tf.matrix_band_part(ones, -1, 0)) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 20)     113280      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_1 (Position (None, None, 20)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, None, 128)    7680        position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5664)         730656      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 851,616\n",
      "Trainable params: 851,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "def create_attention_model(max_sequence_len, total_words):\n",
    "\n",
    "    S_inputs = Input(shape=(None,), dtype='int32')\n",
    "    embeddings = Embedding(total_words, 20)(S_inputs)\n",
    "    embeddings = Position_Embedding()(embeddings) \n",
    "    O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "    O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "    O_seq = Dropout(0.5)(O_seq)\n",
    "    outputs = Dense(total_words, activation='sigmoid')(O_seq)\n",
    "    model = Model(inputs=S_inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model2 = create_attention_model(max_sequence_len, total_words)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "637049/637049 [==============================] - 210s 330us/step - loss: 7.1919\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.19188, saving model to model.hdf5\n",
      "Epoch 2/3\n",
      "637049/637049 [==============================] - 279s 438us/step - loss: 6.9443\n",
      "\n",
      "Epoch 00002: loss improved from 7.19188 to 6.94427, saving model to model.hdf5\n",
      "Epoch 3/3\n",
      "637049/637049 [==============================] - 669s 1ms/step - loss: 6.8016\n",
      "\n",
      "Epoch 00003: loss improved from 6.94427 to 6.80159, saving model to model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d914b4f28>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model2.fit(predictors, label, epochs=3, callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 凄凉徒可知 ,  别见还安家 .\n",
      " 相独鼓闻草 ,  虎湖德存称 .\n",
      "\n",
      "\n",
      " 天台邻可知 ,  别见还安家 .\n",
      " 相独鼓闻草 ,  虎湖德存称 .\n",
      "\n",
      "\n",
      " 南国久知别 ,  见还安家相 .\n",
      " 独鼓闻草可 ,  虎便存字商 .\n",
      "\n",
      "\n",
      " 灵飙动可知 ,  别见还安家 .\n",
      " 相独鼓闻草 ,  虎湖德存称 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p, s = output_poem(4, model2)\n",
    "for i in range(len(p)):\n",
    "    print ('\\n', p[i][:5],', ',p[i][5:10],'.\\n', p[i][10:15],', ', p[i][15:],'.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Poetry_Generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
